{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differential Privacy for Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import dp_stats as dps\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differentially Private PCA\n",
    "---\n",
    "\n",
    "The following tutorial gives one example of how the `dp_pca()` funciton is called. The data samples are randomly drawn i.i.d. from a multivariate Gaussian distribution with a pre-defined mean and covariance matrix. The quality (in terms of the captured energy of the covariance matrix in the reduced dimensional subspace) of the output subspace of the differentially private PCA and non-differentially private PCA is shown as a comparison. \n",
    "\n",
    "The parameters that can be adjusted are:\n",
    "\n",
    "- Epsilon\n",
    "- Sample_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This tutorial gives an example of one way to use the differentially private PCA function\n",
    "# A non-differentially private version of the PCA process will also be run to generate the likeness of the two\n",
    "\n",
    "\n",
    "# This function will be used to randomly generate a data matrix from a multivariate Gaussian distribution\n",
    "def gen_data(Sample_size, k):\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    Sample_size: scalar\n",
    "        total number of test samples to return in data matrix\n",
    "    k : scalar <10\n",
    "        number of dimensions with structure\n",
    "            \n",
    "    Outputs:\n",
    "            trn_data: [trn_size x d]\n",
    "            A: covariance matrix, [d x d]\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    d = 10                       # features\n",
    "    n = Sample_size              # number of samples to generate for each class\n",
    "\n",
    "    # create covariance matrix\n",
    "    A = np.zeros((d,d))\n",
    "    for i in range(d):\n",
    "        if i < k:\n",
    "            A[i,i] = d - i\n",
    "        else:\n",
    "            A[i, i] = 1\n",
    "\n",
    "    # create mean\n",
    "    mean = np.zeros(d)\n",
    "\n",
    "    # generate n samples\n",
    "    data_ = np.random.multivariate_normal(mean, A, n)    # [nxd]\n",
    "\n",
    "    return data_, A\n",
    "\n",
    "# This function will allow the PCA outputs to be interactive\n",
    "def show_pca_qual(Sample_size, k = 5, Epsilon = 1.0):\n",
    "\n",
    "    \n",
    "    # generate the data matrix\n",
    "    data_, A = gen_data(Sample_size, k)    # data_: samples are in rows, A: covariance matrix\n",
    "    \n",
    "    # go through the non-differentially private PCA routine\n",
    "    sigma_control = np.dot(data_.T, data_)     # [d x d] = [d x Sample_size] [Sample_size x d]\n",
    "    U, S, V = np.linalg.svd(sigma_control)\n",
    "    \n",
    "    # grab the first k columns\n",
    "    U_reduc = U[:, :k]\n",
    "    \n",
    "    # find the quality of the PCA control\n",
    "    control_quality = np.trace(np.dot(np.dot(U_reduc.T, A), U_reduc))\n",
    "    \n",
    "    \n",
    "    # go through the differentially private PCA routine\n",
    "    # dp_pca_sn ( data, epsilon=1.0 )  // samples must be in columns\n",
    "    sigma_dp = dps.dp_pca_sn(data_.T, epsilon = Epsilon)\n",
    "    U_dp, S_dp, V_dp = np.linalg.svd(sigma_dp)\n",
    "    \n",
    "    # grab the first k columns\n",
    "    U_dp_reduc = U_dp[:, :k]\n",
    "    \n",
    "    # find the quality of the differentially private PCA method\n",
    "    dp_quality = np.trace(np.dot(np.dot(U_dp_reduc.T, A), U_dp_reduc))\n",
    "    \n",
    "    # output the results\n",
    "    control_txt = \"Non-private Quality: {}\".format(round(control_quality, 4))\n",
    "    display(control_txt)\n",
    "    dp_txt = \"Differentially Private Quality: {}\".format(round(float(dp_quality), 4))\n",
    "    display(dp_txt)\n",
    "\n",
    "interact(show_pca_qual,  k=(1, 10, 1), Sample_size=(50,1000,100), Epsilon=(0.01,3.0,0.01));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data,A = gen_data(500, 5)\n",
    "r,c = data.shape\n",
    "df = pd.DataFrame(data, columns = ['x'+str(i) for i in range(c)])\n",
    "sns.pairplot(data = df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differentially Private SVM Algorithm Tutorial\n",
    "\n",
    "\n",
    "The following tutorial gives one example of how a differentially private pipeline of PCA and SVM functions are called. The data set used is generated from randomly drawn samples from a mulitivariate Gaussian distribution. A non-differentially private pipeline of PCA and SVM is also utilized. This is used to compare the results (classification accuracy) of the differentially private pipeline.\n",
    "\n",
    "###### A sample of the pipeline is shown below:\n",
    "\n",
    "Generate samples --> Perform PCA for dimension reduction --> Perform SVM for classifier training --> Test classifier\n",
    "\n",
    "The parameters that can be adjusted are:\n",
    "\n",
    "- Epsilon_pca\n",
    "- Epsilon_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This tutorial shows one common pipeline for learning a binary classifier\n",
    "\n",
    "\n",
    "# This function is used to randomly generate data samples from a multivariate Gaussian distribution.\n",
    "def gen_data(num_tst_samp, k):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "            num_tst_samp: total number of test samples to return in data matrix\n",
    "            k: number of samples to generate for each class\n",
    "    Outputs:\n",
    "            data: data matrix with samples in rows, [nxd]\n",
    "            labels: n dimensional vector\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    d = 10    # features\n",
    "    n = num_tst_samp\n",
    "\n",
    "    # create covariance matrix\n",
    "    A = np.zeros((d,d))\n",
    "    for i in range(d):\n",
    "        if i < k:\n",
    "            A[i,i] = d - i\n",
    "        else:\n",
    "            A[i, i] = 1\n",
    "\n",
    "    # create mean for class 1\n",
    "    mean1 = -1 * np.ones(d)\n",
    "\n",
    "    # create mean for class 2\n",
    "    mean2 = np.ones(d)\n",
    "\n",
    "    # generate n samples for class 1\n",
    "    cls1_samps = np.random.multivariate_normal(mean1, A, n)    # [nxd]\n",
    "    # generate n samples for class 2\n",
    "    cls2_samps = np.random.multivariate_normal(mean2, A, n)    # [nxd]\n",
    "    return cls1_samps, cls2_samps, A\n",
    "\n",
    "def gen_data2(num_tst_samp, k):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "            num_tst_samp: total number of test samples to return in data matrix\n",
    "            k: number of samples to generate for each class\n",
    "    Outputs:\n",
    "            data: data matrix with samples in rows, [nxd]\n",
    "            labels: n dimensional vector\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    d = 10    # features\n",
    "    n = num_tst_samp\n",
    "\n",
    "    # create covariance matrix\n",
    "    A = np.eye(d)\n",
    "\n",
    "    # create mean for class 1\n",
    "    mean1 = [1]*d\n",
    "\n",
    "    # create mean for class 2\n",
    "    mean2 = [-1]*d\n",
    "    mean2[k:] = [1]*(d-k)\n",
    "\n",
    "    # generate n samples for class 1\n",
    "    cls1_samps = np.random.multivariate_normal(mean1, A, n)    # [nxd]\n",
    "    # generate n samples for class 2\n",
    "    cls2_samps = np.random.multivariate_normal(mean2, A, n)    # [nxd]\n",
    "    return cls1_samps, cls2_samps, A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used to randomly mix the two class samples and return training and testing data\n",
    "def sample_selection(data_cls1, data_cls2, trn_size):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "            data_cls1: class 1 data, samples are in rows [nxd]\n",
    "            data_cls2: class 2 data, samples are in rows [nxd]\n",
    "            trn_size: number of samples to use for training data, integer\n",
    "            tst_size: number of samples to use for testing data, integer\n",
    "    Outputs:\n",
    "            trn_data: [trn_size x d]\n",
    "            trn_labels:[trn_size]\n",
    "            tst_data: [tst_size x d]\n",
    "            tst_labels: [tst_size]\n",
    "    \"\"\"\n",
    "\n",
    "    N = len(data_cls1)\n",
    "    ind = np.random.permutation(N)\n",
    "    trn_data1 = data_cls1[ind[:trn_size],:]\n",
    "    trn_data2 = data_cls2[ind[:trn_size],:]\n",
    "    tst_data1 = data_cls1[ind[trn_size:N],:]\n",
    "    tst_data2 = data_cls2[ind[trn_size:N],:]\n",
    "\n",
    "    trn_data = np.concatenate((trn_data1, trn_data2),axis=0)\n",
    "    trn_labels = np.concatenate( (np.ones(trn_size), -1*np.ones(trn_size)),axis=0 )\n",
    "\n",
    "    tst_data = np.concatenate((tst_data1, tst_data2),axis=0)\n",
    "    tst_labels = np.concatenate((np.ones(N-trn_size),-1*np.ones(N-trn_size)),axis=0)\n",
    "\n",
    "    return trn_data, trn_labels, tst_data, tst_labels\n",
    "\n",
    "# this function will score the differentially private classifier\n",
    "def test_dp_clf(data_tst, labels_tst, dp_clf):\n",
    "    import numpy as np\n",
    "    \n",
    "    # loop through the data and record wrong answers\n",
    "    n = len(labels_tst)\n",
    "    tot_err = 0\n",
    "    \n",
    "    for i in range(n):\n",
    "        dp_ans = np.dot(data_tst[i,:], dp_clf)\n",
    "        if labels_tst[i] < 0:\n",
    "            if dp_ans >= 0:\n",
    "                tot_err += 1\n",
    "        else:\n",
    "            if dp_ans <= 0:\n",
    "                tot_err += 1\n",
    "    accuracy = (n - tot_err) * 1.0 / (n * 1.0)\n",
    "    return accuracy\n",
    "\n",
    "# This function is used give the pipeline interactive control.\n",
    "def svm_pipeline(Epsilon_pca = 0.5, Epsilon_svm = 0.5, k=4,sample_size=500):\n",
    "\n",
    "    \n",
    "    # first generate the training and testing data\n",
    "    cls1, cls2, A = gen_data2(num_tst_samp = sample_size, k = k)\n",
    "    \n",
    "    trn_data, trn_labels, tst_data, tst_labels = sample_selection(cls1, cls2, trn_size = int(np.floor(sample_size*.8)))\n",
    "    \n",
    "    # go through the non-differentially private PCA routine\n",
    "    sigma_control = np.dot(trn_data.T, trn_data)     # [d x d] = [d x Sample_size] [Sample_size x d]\n",
    "    U, S, V = np.linalg.svd(sigma_control)\n",
    "    \n",
    "    # grab the first k columns\n",
    "    U_reduc = U[:, :4]      # [d x k]\n",
    "    \n",
    "    # project the data onto the k subspace\n",
    "    trn_data_reduc = np.dot(trn_data, U_reduc)\n",
    "    \n",
    "    # go through SVM routine\n",
    "    clf = svm.SVC(kernel = 'linear', gamma = 0.01, C = 10)\n",
    "    clf.fit(trn_data_reduc, trn_labels)\n",
    "    \n",
    "    # reduce testing data to use to score the control classifier\n",
    "    tst_data_reduc = np.dot(tst_data, U_reduc)    # [d x k]\n",
    "    control_score = clf.score(tst_data_reduc, tst_labels)\n",
    "    #print(control_score)\n",
    "    \n",
    "    # go through differentially private pipeline\n",
    "    # dp_pca_sn ( data, epsilon=1.0 )  // samples must be in columns\n",
    "    sigma_dp = dps.dp_pca_ag(tst_data.T, epsilon = Epsilon_pca)\n",
    "    U_dp, S_dp, V_dp = np.linalg.svd(sigma_dp)\n",
    "    \n",
    "    # grab the first k columns\n",
    "    U_dp_reduc = U_dp[:, :4]\n",
    "    \n",
    "    # project the data\n",
    "    dp_trn_data = np.dot(trn_data, U_dp_reduc)\n",
    "    \n",
    "    # go through differentially private svm routine\n",
    "    # dp_svm(data, labels, method='obj', epsilon=0.1, Lambda = 0.01, h = 0.5)\n",
    "    clf_dp = dps.dp_svm(dp_trn_data, trn_labels, epsilon = Epsilon_svm)\n",
    "    \n",
    "    # reduce the testing data\n",
    "    tst_dp_data = np.dot(tst_data, U_dp_reduc)\n",
    "    \n",
    "    # test the differentially private classifier\n",
    "    dp_score = test_dp_clf(tst_dp_data, tst_labels, clf_dp)\n",
    "    #print(dp_score)\n",
    "    \n",
    "    # output the results\n",
    "    control_txt = \"Non-private Quality: {}\".format(round(control_score, 4))\n",
    "    display(control_txt)\n",
    "    dp_txt = \"Differentially Private Quality: {}\".format(round(dp_score, 4))\n",
    "    display(dp_txt)\n",
    "    \n",
    "interact(svm_pipeline, Epsilon_pca=(0.01,1.0,0.05), Epsilon_svm=(0.01,1.0, 0.05), k = (1,10,1),sample_size = (10,1000,5));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [-1]*10\n",
    "a[5:] = [1]*(10-5)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
