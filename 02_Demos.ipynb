{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import dp_stats as dps\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differentially Private Mean\n",
    "---\n",
    "\n",
    "The following tutorial gives one example of how the `dp_mean()` function is called. The data samples are randomly drawn from a Gaussian distribution. The output of the `dp_mean()` function will be compared to a non-differentially private version of the sample mean: $\\bar{x}=\\frac{1}{n}\\sum_{i=1}^{n}x_i$. \n",
    "\n",
    "The parameters that can be adjusted are:\n",
    "\n",
    "- Epsilon\n",
    "- Delta\n",
    "- Sample_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This tutorial gives an example of using the dp_mean() function\n",
    "# The true sample mean and differentially private mean of the data vector will be displayed for comparison\n",
    "\n",
    "max_sample_size = 100000\n",
    "\n",
    "# generate a sample data vector\n",
    "data_ = np.random.normal(loc = 0, scale = 1.0, size = max_sample_size)\n",
    "\n",
    "# This function will allow the outputs of the means to be interactive\n",
    "def show_mean(data,Epsilon=1.0, Delta = 0.1, Sample_size = 100):\n",
    "    \n",
    "    data = data[:Sample_size]\n",
    "    \n",
    "    # restric data vector to be positive and within the range [0, 1]\n",
    "    data_ = abs(data)\n",
    "    data_ = data_.clip(min = 0, max = 1.0)\n",
    "\n",
    "    # find the non-differentially private mean of the generated data\n",
    "    mean_control = (np.sum(data_) * 1.0) / (Sample_size * 1.0)\n",
    "    \n",
    "    # find the differentially private mean of the generated data\n",
    "    # dp_mean( data_vect, epsilon=1.0, delta=0.1 )\n",
    "    mean_dp = dps.dp_mean(data_, epsilon = Epsilon, delta = Delta)\n",
    "    \n",
    "    # output the control and differentially private mean\n",
    "    control_txt = 'Non-private Mean: {}'.format(round(mean_control, 4))\n",
    "    display(control_txt)\n",
    "    dp_txt = 'Differentially Private Mean: {}'.format(round(float(mean_dp), 4))\n",
    "    display(dp_txt)\n",
    "    \n",
    "show_mean_interact = lambda Epsilon, Delta, Sample_size: show_mean(data_,Epsilon, Delta, Sample_size )\n",
    "\n",
    "interact(show_mean_interact, Epsilon=(0.01,3,0.01), Delta=(0.01,0.5,0.01), Sample_size=(100,max_sample_size,500));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each run of the cell generates a new dataset, changing the sample size uses a different amount of the same sample, changing other parameters re-privatizes the same dataset.\n",
    "\n",
    "It can be noted from the outputs that the differentially private mean will roughly come closer to the actual sample mean when the sample size becomes larger with fixed privacy level, or the privacy level becomes small (Epsilon being large) with fixed sample size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differentially Private Variance\n",
    "---\n",
    "\n",
    "The following tutorial gives one example of how the `dp_var()` funciton is called. The data samples are randomly drawn from a Gaussian distribution. The output of the `dp_var()` function will be compared to a non-differentially private version of the sample variance: $\\sigma^2=\\frac{1}{n}\\sum_{i=1}^{n}(x_i-\\bar{x})^2$, where $\\bar{x}$ is the sample mean. \n",
    "\n",
    "The parameters that can be adjusted are:\n",
    "\n",
    "- Epsilon\n",
    "- Delta\n",
    "- Sample_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# generate a sample data vector\n",
    "data_ = np.random.normal(loc = 0, scale = 1.0, size = max_sample_size)\n",
    "\n",
    "# This function will allow the output of the variances to be interactive\n",
    "def show_variance(data,Epsilon=1.0, Delta = 0.1, Sample_size = 100):\n",
    "    data_ = data[:Sample_size]\n",
    "    \n",
    "    # restric data vector to be positive and within the range [0, 1]\n",
    "    data_ = abs(data_)\n",
    "    data_ = data_.clip(min = 0, max = 1.0)\n",
    "\n",
    "    # find the non-differentially private mean of the generated data\n",
    "    mean_control = (np.sum(data_) * 1.0) / (Sample_size * 1.0)\n",
    "    var_control = 0\n",
    "    for i in range(Sample_size):\n",
    "        var_control += (data_[i] - mean_control)**2\n",
    "    var_control = var_control / (Sample_size * 1.0)\n",
    "    \n",
    "    # find the differentially private variance of the generated data\n",
    "    # dp_var( data_vector,epsilon=1.0,delta=0.1 )\n",
    "    var_dp = dps.dp_var(data_, epsilon = Epsilon, delta = Delta)\n",
    "    \n",
    "    # output the control and differentially private variance\n",
    "    control_txt = \"Non-private Variance: {}\".format(round(var_control, 4))\n",
    "    display(control_txt)\n",
    "    dp_txt = \"Differentially Private Variance: {}\".format(round(float(var_dp), 4))\n",
    "    display(dp_txt)\n",
    "    \n",
    "show_variance_interact = lambda Epsilon, Delta, Sample_size: show_variance(data_,Epsilon=1.0, Delta = 0.1, Sample_size = 100)\n",
    "\n",
    "\n",
    "interact(show_variance_interact, Epsilon=(0.01,3.0,0.01), Delta=(0.01,0.5,0.01), Sample_size=(100,max_sample_size,100));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each run of the cell generates a new dataset, changing the sample size uses a different amount of the same sample, changing other parameters re-privatizes the same dataset.\n",
    "\n",
    "It can be noted from the outputs that the differentially private variance will roughly come closer to the actual sample variance when the sample size becomes larger with fixed privacy level, or the privacy level becomes small (Epsilon being large) with fixed sample size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Differentially Private PCA\n",
    "---\n",
    "\n",
    "The following tutorial gives one example of how the `dp_pca()` funciton is called. The data samples are randomly drawn i.i.d. from a multivariate Gaussian distribution with a pre-defined mean and covariance matrix. The quality (in terms of the captured energy of the covariance matrix in the reduced dimensional subspace) of the output subspace of the differentially private PCA and non-differentially private PCA is shown as a comparison. \n",
    "\n",
    "The parameters that can be adjusted are:\n",
    "\n",
    "- Epsilon\n",
    "- Sample_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This tutorial gives an example of one way to use the differentially private PCA function\n",
    "# A non-differentially private version of the PCA process will also be run to generate the likeness of the two\n",
    "\n",
    "\n",
    "# This function will be used to randomly generate a data matrix from a multivariate Gaussian distribution\n",
    "def gen_data(Sample_size, k):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "            Sample_size: total number of test samples to return in data matrix\n",
    "    Outputs:\n",
    "            trn_data: [trn_size x d]\n",
    "            A: covariance matrix, [d x d]\n",
    "    \"\"\"\n",
    "    \n",
    "    import numpy as np\n",
    "\n",
    "    d = 10                       # features\n",
    "    n = Sample_size              # number of samples to generate for each class\n",
    "\n",
    "    # create covariance matrix\n",
    "    A = np.zeros((d,d))\n",
    "    for i in range(d):\n",
    "        if i < k:\n",
    "            A[i,i] = d - i\n",
    "        else:\n",
    "            A[i, i] = 1\n",
    "\n",
    "    # create mean\n",
    "    mean = np.zeros(d)\n",
    "\n",
    "    # generate n samples\n",
    "    data_ = np.random.multivariate_normal(mean, A, n)    # [nxd]\n",
    "\n",
    "    return data_, A\n",
    "\n",
    "# This function will allow the PCA outputs to be interactive\n",
    "def show_pca_qual(Sample_size, k = 5, Epsilon = 1.0):\n",
    "    import numpy as np\n",
    "    import dp_stats as dps\n",
    "    \n",
    "    # generate the data matrix\n",
    "    data_, A = gen_data(Sample_size, k)    # data_: samples are in rows, A: covariance matrix\n",
    "    \n",
    "    # go through the non-differentially private PCA routine\n",
    "    sigma_control = np.dot(data_.T, data_)     # [d x d] = [d x Sample_size] [Sample_size x d]\n",
    "    U, S, V = np.linalg.svd(sigma_control)\n",
    "    \n",
    "    # grab the first k columns\n",
    "    U_reduc = U[:, :k]\n",
    "    \n",
    "    # find the quality of the PCA control\n",
    "    control_quality = np.trace(np.dot(np.dot(U_reduc.T, A), U_reduc))\n",
    "    \n",
    "    \n",
    "    # go through the differentially private PCA routine\n",
    "    # dp_pca_sn ( data, epsilon=1.0 )  // samples must be in columns\n",
    "    sigma_dp = dps.dp_pca_sn(data_.T, epsilon = Epsilon)\n",
    "    U_dp, S_dp, V_dp = np.linalg.svd(sigma_dp)\n",
    "    \n",
    "    # grab the first k columns\n",
    "    U_dp_reduc = U_dp[:, :k]\n",
    "    \n",
    "    # find the quality of the differentially private PCA method\n",
    "    dp_quality = np.trace(np.dot(np.dot(U_dp_reduc.T, A), U_dp_reduc))\n",
    "    \n",
    "    # output the results\n",
    "    control_txt = \"Non-private Quality: {}\".format(round(control_quality, 4))\n",
    "    display(control_txt)\n",
    "    dp_txt = \"Differentially Private Quality: {}\".format(round(float(dp_quality), 4))\n",
    "    display(dp_txt)\n",
    "\n",
    "interact(show_pca_qual, Sample_size=(50,1000,100), k=(1, 10, 1), Epsilon=(0.01,3.0,0.01));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differentially Private SVM Algorithm Tutorial\n",
    "\n",
    "\n",
    "The following tutorial gives one example of how a differentially private pipeline of PCA and SVM functions are called. The data set used is generated from randomly drawn samples from a mulitivariate Gaussian distribution. A non-differentially private pipeline of PCA and SVM is also utilized. This is used to compare the results (classification accuracy) of the differentially private pipeline.\n",
    "\n",
    "###### A sample of the pipeline is shown below:\n",
    "\n",
    "Generate samples --> Perform PCA for dimension reduction --> Perform SVM for classifier training --> Test classifier\n",
    "\n",
    "The parameters that can be adjusted are:\n",
    "\n",
    "- Epsilon_pca\n",
    "- Epsilon_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This tutorial shows one common pipeline for learning a binary classifier\n",
    "\n",
    "\n",
    "# This function is used to randomly generate data samples from a multivariate Gaussian distribution.\n",
    "def gen_data(num_tst_samp, k):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "            num_tst_samp: total number of test samples to return in data matrix\n",
    "            n: number of samples to generate for each class\n",
    "    Outputs:\n",
    "            data: data matrix with samples in rows, [nxd]\n",
    "            labels: n dimensional vector\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import dp_stats as dps\n",
    "\n",
    "    d = 10    # features\n",
    "    n = num_tst_samp\n",
    "\n",
    "    # create covariance matrix\n",
    "    A = np.zeros((d,d))\n",
    "    for i in range(d):\n",
    "        if i < k:\n",
    "            A[i,i] = d - i\n",
    "        else:\n",
    "            A[i, i] = 1\n",
    "\n",
    "    # create mean for class 1\n",
    "    mean1 = -1 * np.ones(d)\n",
    "\n",
    "    # create mean for class 2\n",
    "    mean2 = np.ones(d)\n",
    "\n",
    "    # generate n samples for class 1\n",
    "    cls1_samps = np.random.multivariate_normal(mean1, A, n)    # [nxd]\n",
    "    # generate n samples for class 2\n",
    "    cls2_samps = np.random.multivariate_normal(mean2, A, n)    # [nxd]\n",
    "    return cls1_samps, cls2_samps, A\n",
    "\n",
    "# This function is used to randomly mix the two class samples and return training and testing data\n",
    "def sample_selection(data_cls1, data_cls2, trn_size, N):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "            data_cls1: class 1 data, samples are in rows [nxd]\n",
    "            data_cls2: class 2 data, samples are in rows [nxd]\n",
    "            trn_size: number of samples to use for training data, integer\n",
    "            tst_size: number of samples to use for testing data, integer\n",
    "    Outputs:\n",
    "            trn_data: [trn_size x d]\n",
    "            trn_labels:[trn_size]\n",
    "            tst_data: [tst_size x d]\n",
    "            tst_labels: [tst_size]\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    ind = np.random.permutation(N)\n",
    "    trn_data1 = data_cls1[ind[:trn_size],:]\n",
    "    trn_data2 = data_cls2[ind[:trn_size],:]\n",
    "    tst_data1 = data_cls1[ind[trn_size:N],:]\n",
    "    tst_data2 = data_cls2[ind[trn_size:N],:]\n",
    "\n",
    "    trn_data = np.concatenate((trn_data1, trn_data2),axis=0)\n",
    "    trn_labels = np.concatenate( (np.ones(trn_size), -1*np.ones(trn_size)),axis=0 )\n",
    "\n",
    "    tst_data = np.concatenate((tst_data1, tst_data2),axis=0)\n",
    "    tst_labels = np.concatenate((np.ones(N-trn_size),-1*np.ones(N-trn_size)),axis=0)\n",
    "\n",
    "    return trn_data, trn_labels, tst_data, tst_labels\n",
    "\n",
    "# this function will score the differentially private classifier\n",
    "def test_dp_clf(data_tst, labels_tst, dp_clf):\n",
    "    import numpy as np\n",
    "    \n",
    "    # loop throught the data and record wrong answers\n",
    "    n = len(labels_tst)\n",
    "    tot_err = 0\n",
    "    \n",
    "    for i in range(n):\n",
    "        dp_ans = np.dot(data_tst[i,:], dp_clf)\n",
    "        if labels_tst[i] < 0:\n",
    "            if dp_ans >= 0:\n",
    "                tot_err += 1\n",
    "        else:\n",
    "            if dp_ans <= 0:\n",
    "                tot_err += 1\n",
    "    accuracy = (n - tot_err) * 1.0 / (n * 1.0)\n",
    "    return accuracy\n",
    "\n",
    "# This function is used give the pipeline interactive control.\n",
    "def svm_pipeline(Epsilon_pca = 0.5, Epsilon_svm = 0.5):\n",
    "    from sklearn import svm\n",
    "    import numpy as np\n",
    "    import dp_stats as dps\n",
    "    \n",
    "    # first generate the training and testing data\n",
    "    cls1, cls2, A = gen_data(num_tst_samp = 2200, k = 4)\n",
    "    \n",
    "    trn_data, trn_labels, tst_data, tst_labels = sample_selection(cls1, cls2, trn_size = 1000, N = 1100)\n",
    "    \n",
    "    # go through the non-differentially private PCA routine\n",
    "    sigma_control = np.dot(trn_data.T, trn_data)     # [d x d] = [d x Sample_size] [Sample_size x d]\n",
    "    U, S, V = np.linalg.svd(sigma_control)\n",
    "    \n",
    "    # grab the first k columns\n",
    "    U_reduc = U[:, :4]      # [d x k]\n",
    "    \n",
    "    # project the data onto the k subspace\n",
    "    trn_data_reduc = np.dot(trn_data, U_reduc)\n",
    "    \n",
    "    # go through SVM routine\n",
    "    clf = svm.SVC(kernel = 'linear', gamma = 0.01, C = 10)\n",
    "    clf.fit(trn_data_reduc, trn_labels)\n",
    "    \n",
    "    # reduce testing data to use to score the control classifier\n",
    "    tst_data_reduc = np.dot(tst_data, U_reduc)    # [d x k]\n",
    "    control_score = clf.score(tst_data_reduc, tst_labels)\n",
    "    #print(control_score)\n",
    "    \n",
    "    # go through differentially private pipeline\n",
    "    # dp_pca_sn ( data, epsilon=1.0 )  // samples must be in columns\n",
    "    sigma_dp = dps.dp_pca_ag(tst_data.T, epsilon = Epsilon_pca)\n",
    "    U_dp, S_dp, V_dp = np.linalg.svd(sigma_dp)\n",
    "    \n",
    "    # grab the first k columns\n",
    "    U_dp_reduc = U_dp[:, :4]\n",
    "    \n",
    "    # project the data\n",
    "    dp_trn_data = np.dot(trn_data, U_dp_reduc)\n",
    "    \n",
    "    # go through differentially private svm routine\n",
    "    # dp_svm(data, labels, method='obj', epsilon=0.1, Lambda = 0.01, h = 0.5)\n",
    "    clf_dp = dps.dp_svm(dp_trn_data, trn_labels, epsilon = Epsilon_svm)\n",
    "    \n",
    "    # reduce the testing data\n",
    "    tst_dp_data = np.dot(tst_data, U_dp_reduc)\n",
    "    \n",
    "    # test the differentially private classifier\n",
    "    dp_score = test_dp_clf(tst_dp_data, tst_labels, clf_dp)\n",
    "    #print(dp_score)\n",
    "    \n",
    "    # output the results\n",
    "    control_txt = \"Non-private Quality: {}\".format(round(control_score, 4))\n",
    "    display(control_txt)\n",
    "    dp_txt = \"Differentially Private Quality: {}\".format(round(dp_score, 4))\n",
    "    display(dp_txt)\n",
    "    \n",
    "interact(svm_pipeline, Epsilon_pca=(0.01,1.0,0.05), Epsilon_svm=(0.01,1.0, 0.05));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the pipeline above takes some time, so move the bars slowly to see results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differentially Private Histogram\n",
    "---\n",
    "\n",
    "The following tutorial gives one example of how the `dp_hist()` function is called. The data samples are randomly drawn from a Gaussian distribution. The output of the `dp_hist()` function will be used to populate a bar graph and show the empirical distribution of the data. In addition, a non-private version of the histogram will be shown as a comparison.\n",
    "\n",
    "The parameters that can be adjusted are:\n",
    "\n",
    "- Epsilon\n",
    "- Sample_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will allow the output graphs to be interactive\n",
    "def plthist(Epsilon=1.0, Sample_size = 100):\n",
    "    # generate a sample data vector\n",
    "    data_ = np.random.normal(loc = 0, scale = 1.0, size = Sample_size)\n",
    "\n",
    "    # get the histogram count and bin edges from the differentially private histogram\n",
    "    # dp_hist ( data, num_bins=10, epsilon=1.0, delta=0.1, histtype = 'continuous' )\n",
    "    dp_hist_counts, bin_edges = dps.dp_hist(data = data_, num_bins = 10, epsilon = Epsilon, delta = 0.1)\n",
    "\n",
    "    dp_hist_counts = dp_hist_counts.clip(min = 0)    # number of items in bin can't be negative\n",
    "    bin_edges_ = bin_edges[:len(dp_hist_counts)]    # grab only the left edge for each bin\n",
    "\n",
    "    # get the width for each of the bins\n",
    "    width_ = []\n",
    "    for i in range(len(bin_edges) - 1):\n",
    "        app = bin_edges[i + 1] - bin_edges[i]\n",
    "        width_.append(app)\n",
    "\n",
    "    # plot the non-differentially private version of the histogram\n",
    "    plt.hist(data_, bins=10)\n",
    "    plt.title('Non-differentially Private Histogram')\n",
    "    plt.xlabel('Data Distribution')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # plot the differentially private version of the histogram\n",
    "    plt.bar(bin_edges_, dp_hist_counts, width_)\n",
    "    plt.title('Differentially Private Histogram')\n",
    "    plt.xlabel('Data Distribution')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "interact(plthist, Epsilon=(0.1,5.0,0.1), Sample_size=(100,10000,100));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
